{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2744f214",
   "metadata": {},
   "source": [
    "# Prompt engineering for CoNNL-U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e9402",
   "metadata": {},
   "source": [
    "Here's an example API query. To run this yourself you need to run `export OPENAI_API_KEY_TREEBANKS=\"your api key\"` in the terminal with the API key from your own OpenAI account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [03:00, 36.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThetta\tthetta\tPRON\t_\tCase=Nom|Gender=Neut|Number=Sing|PronType=Dem\t4\tnsubj\t4:nsubj\t_\n",
      "2\tÃ¤r\tvara\tAUX\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t4\tcop\t4:cop\t_\n",
      "3\tinte\tinte\tPART\t_\tPolarity=Neg\t2\tneg\t2:neg\t_\n",
      "4\tbegynnilsen\tbegynnilsen\tNOUN\t_\tCase=Nom|Definite=Def|Gender=Com|Number=Sing\t0\troot\t0:root\t_\n",
      "5\taff\taff\tADP\t_\tAdpType=Prep\t10\tcase\t10:case\t_\n",
      "6\tJesu\tJesu\tPROPN\t_\tCase=Gen|Gender=Masc|Number=Sing\t7\tflat\t7:flat|10:nmod:poss\t_\n",
      "7\tChristi\tChristi\tPROPN\t_\tCase=Gen|Gender=Masc|Number=Sing\t10\tnmod:poss\t10:nmod:poss\t_\n",
      "8\tgudz\tgudz\tPROPN\t_\tCase=Gen|Gender=Masc|Number=Sing\t9\tnmod:poss\t9:nmod:poss|10:nmod:poss\t_\n",
      "9\tsons\tsons\tNOUN\t_\tCase=Gen|Gender=Masc|Number=Sing\t10\tnmod:poss\t10:nmod:poss\t_\n",
      "10\teuangelio\teuangelio\tNOUN\t_\tCase=Nom|Gender=Neut|Number=Sing\t4\tnmod\t4:nmod\t_\n",
      "11\t.\t.\tPUNCT\t_\t_\t4\tpunct\t4:punct\t_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.pipeline import pipeline\n",
    "\n",
    "input_sentence = \"Thetta Ã¤r inte begynnilsen aff Jesu Christi gudz sons euangelio.\"\n",
    "conllu = pipeline(input_sentence, model=\"gpt-5-mini-2025-08-07\")\n",
    "print(conllu)\n",
    "\n",
    "if not os.path.exists(\"output\"):\n",
    "    os.makedirs(\"output\")\n",
    "outname = f\"output/parsed_{input_sentence.split()[0]}.conllu\"\n",
    "with open(outname, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(conllu + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d4b9b",
   "metadata": {},
   "source": [
    "We can quickly check validity using the python `conllu` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbfbe0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoNNL-U valid ðŸ¥³\n"
     ]
    }
   ],
   "source": [
    "from src.pipeline import is_valid_conllu\n",
    "\n",
    "#outname = f\"output/parsed_{input_sentence.split()[0]}.conllu\"\n",
    "outname = f\"output/parsed.conllu\"\n",
    "validity = is_valid_conllu(outname)\n",
    "if validity:\n",
    "    print(\"CoNNL-U valid ðŸ¥³\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb02ed",
   "metadata": {},
   "source": [
    "## The Batch API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f2400",
   "metadata": {},
   "source": [
    "Before running on a large file from Svensk diakronisk korpus, you can estimate the cost roughly (based on using the Batch API, which is 50% cheaper). Caveat emptor, though, your credit card is on its own! To be absolutely safe, you can set a limit in your project settings.\n",
    "\n",
    "The algorithm counts tokens from all `# text =` fields in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c67c0e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input tokens: 79705\n",
      "Approximate input cost: $0.009963\n",
      "Approximate output cost: $0.034200\n",
      "Approximate total cost: $0.044163\n"
     ]
    }
   ],
   "source": [
    "from src.count_tokens import count_total_tokens_and_cost\n",
    "\n",
    "path = \"data/svediakorp-rel108-Mar26SLundversion.tsv\"\n",
    "tokens, input_cost, output_cost = count_total_tokens_and_cost(path)\n",
    "print(f\"Total input tokens: {tokens}\")\n",
    "print(f\"Approximate input cost: ${input_cost:.6f}\")\n",
    "print(f\"Approximate output cost: ${output_cost:.6f}\")\n",
    "print(f\"Approximate total cost: ${input_cost + output_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a13449a",
   "metadata": {},
   "source": [
    "Irritatingly enough, we have to make one batch jsonl per task, submit it, and then incorporate the results into the jsonl of the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069304aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prepare_task1_responses_batch_jsonl] wrote 171 requests -> batches/batch_task1.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.batching import prepare_task1_responses_batch_jsonl\n",
    "\n",
    "prepare_task1_responses_batch_jsonl(\"data/svediakorp-rel108-Mar26SLundversion.conllu\", \"batches/batch_task1.jsonl\", model=\"gpt-5-mini-2025-08-07\", max_output_tokens=4096)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
